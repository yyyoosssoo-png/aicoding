#!/usr/bin/env python3
"""
NCT ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° ê°•ì œ ì£¼ì… ìŠ¤í¬ë¦½íŠ¸

ëª©ì : 4ê°œ íšŒì°¨ì˜ NCT ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„°ë¥¼ ê¸°ì¡´ Courseì— ë§µí•‘í•˜ì—¬ 
      Responses ë° Respondents ì‹œíŠ¸ì— ì£¼ì…

ì‹¤í–‰ ë°©ë²•:
    python inject_responses.py

ì£¼ì˜ì‚¬í•­:
    - ì‹¤í–‰ ì „ Responses ë° Respondents ì‹œíŠ¸ì˜ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤
    - ë°±ì—… í•„ìš” ì‹œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ë¨¼ì € ë³µì‚¬í•˜ì„¸ìš”
"""

import os
import sys
import io
import hashlib
import re
from datetime import datetime, timezone
from typing import Dict, List, Tuple
import time

import pandas as pd
import gspread

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from gsheets_utils import (
    get_client,
    open_or_create_spreadsheet,
    save_respondent,
    save_response_v2,
    ensure_survey_items_from_headers,
    ensure_course_item_mapping,
    delete_course_item_mappings,
)
from survey_app import normalize_company_name, generate_response_id, generate_respondent_id, generate_batch_id


# ============================================================================
# ì„¤ì •: Course IDì™€ íŒŒì¼ëª… ë§µí•‘
# ============================================================================

COURSE_FILE_MAPPING = [
    {
        "course_id": "CARD-1c9x",
        "file_path": "êµìœ¡ì„¤ë¬¸ëŒ€ì‰¬ë³´ë“œ/NCT 1íšŒì°¨ ì„¤ë¬¸ì§€.csv",
        "description": "Next Chip Talk 1íšŒì°¨ - ë¹›ì´ ì—¬ëŠ” ë°˜ë„ì²´ì˜ ë¯¸ë˜"
    },
    {
        "course_id": "CARD-1cqk",
        "file_path": "êµìœ¡ì„¤ë¬¸ëŒ€ì‰¬ë³´ë“œ/NCT 2íšŒì°¨ ì„¤ë¬¸ì§€.csv",
        "description": "Next Chip Talk 2íšŒì°¨ - ë°˜ë„ì²´, ìœ ë¦¬ë¥¼ í’ˆë‹¤"
    },
    {
        "course_id": "CARD-1cyn",
        "file_path": "êµìœ¡ì„¤ë¬¸ëŒ€ì‰¬ë³´ë“œ/NCT 3íšŒì°¨ ì„¤ë¬¸ì§€.csv",
        "description": "Next Chip Talk 3íšŒì°¨ - AIì˜ ë‡Œë¥¼ ì„¤ê³„í•˜ë‹¤"
    },
    {
        "course_id": "CARD-1ddq",
        "file_path": "êµìœ¡ì„¤ë¬¸ëŒ€ì‰¬ë³´ë“œ/NCT 4íšŒì°¨ ì„¤ë¬¸ì§€.csv",
        "description": "Next Chip Talk 4íšŒì°¨ - Next AND Necessity About New Direction"
    },
]


# PII/ë©”íƒ€ë°ì´í„° ì—´ ë§¤í•‘ (í—¤ë” í…ìŠ¤íŠ¸ -> Respondents í•„ë“œ)
PII_COLUMN_MAPPING = {
    "ì†Œì† íšŒì‚¬": "company",
    "ì†Œì† íšŒì‚¬ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”": "company",
    "ì†Œì† íšŒì‚¬ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.": "company",
    "ì§êµ°": "job_role",
    "ë³¸ì¸ì˜ ì§êµ°ì„ ì„ íƒí•´ì£¼ì„¸ìš”": "job_role",
    "ë³¸ì¸ì˜ ì§êµ°ì„ ì„ íƒí•´ì£¼ì„¸ìš”.": "job_role",
    "ì—°ì°¨": "tenure_years",
    "ë³¸ì¸ì˜ ì—°ì°¨ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”": "tenure_years",
    "ë³¸ì¸ì˜ ì—°ì°¨ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.": "tenure_years",
    "ì„±í•¨": "name",
    "ì„±í•¨ì„ ì‘ì„±í•´ì£¼ì„¸ìš”": "name",
    "ì„±í•¨ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.": "name",
    "ì „í™”ë²ˆí˜¸": "phone",
    "ì „í™”ë²ˆí˜¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”": "phone",
}


# ============================================================================
# ì‹œíŠ¸ í´ë¦°ì§• í•¨ìˆ˜
# ============================================================================

def clear_sheet_data(spreadsheet: gspread.Spreadsheet, sheet_name: str):
    """
    íŠ¹ì • ì‹œíŠ¸ì˜ ëª¨ë“  ë°ì´í„° í–‰ì„ ì‚­ì œ (í—¤ë” ì œì™¸)
    
    Args:
        spreadsheet: Google Spreadsheet ê°ì²´
        sheet_name: í´ë¦°ì§•í•  ì‹œíŠ¸ ì´ë¦„
    """
    try:
        ws = spreadsheet.worksheet(sheet_name)
        all_values = ws.get_all_values()
        
        if len(all_values) <= 1:
            print(f"   âœ… {sheet_name}: ì´ë¯¸ ë¹„ì–´ìˆìŒ (í—¤ë”ë§Œ ì¡´ì¬)")
            return
        
        # í—¤ë” ì œì™¸í•œ ëª¨ë“  í–‰ ì‚­ì œ
        num_rows_to_delete = len(all_values) - 1
        ws.delete_rows(2, len(all_values))
        
        print(f"   âœ… {sheet_name}: {num_rows_to_delete}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"   âš ï¸ {sheet_name} í´ë¦°ì§• ì‹¤íŒ¨: {str(e)}")


# ============================================================================
# CSV/XLSX íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì²˜ë¦¬)
# ============================================================================

def read_response_file(file_path: str) -> pd.DataFrame:
    """
    CSV ë˜ëŠ” XLSX íŒŒì¼ì„ ì½ì–´ DataFrame ë°˜í™˜
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        
    Returns:
        pd.DataFrame: ì½ì€ ë°ì´í„°
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    # ğŸš¨ í•µì‹¬ ìˆ˜ì •: íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ ë¨¼ì € í™•ì¸ (í™•ì¥ìë³´ë‹¤ ìš°ì„ )
    with open(file_path, 'rb') as f:
        magic = f.read(4)
    
    is_zip_based = magic[:2] == b'PK'  # ZIP/XLSX ì‹œê·¸ë‹ˆì²˜
    
    # ZIP ê¸°ë°˜ íŒŒì¼ (XLSX)ì´ë©´ ë¬´ì¡°ê±´ Excelë¡œ ì½ê¸°
    if is_zip_based:
        print(f"   ğŸ’¡ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸: XLSX í˜•ì‹ (ZIP ê¸°ë°˜)")
        try:
            df = pd.read_excel(file_path, header=0, dtype=str, engine='openpyxl')
            print(f"   âœ… Excel íŒŒì¼ ì½ê¸° ì„±ê³µ: {len(df)} í–‰")
            return df
        except Exception as e:
            print(f"   âŒ Excel ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    # ZIP ê¸°ë°˜ì´ ì•„ë‹ˆë©´ CSVë¡œ ì‹œë„
    print(f"   ğŸ’¡ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸: CSV í˜•ì‹")
    encodings = ['utf-8-sig', 'cp949', 'euc-kr', 'utf-8', 'latin-1']
    
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, header=0, encoding=encoding, dtype=str)
            print(f"   âœ… CSV íŒŒì¼ ì½ê¸° ì„±ê³µ ({encoding}): {len(df)} í–‰")
            return df
        except Exception:
            continue
    
    raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")


# ============================================================================
# PII ì¶”ì¶œ ë° ì •ê·œí™”
# ============================================================================

def extract_pii_from_row(row: pd.Series, header_to_field: Dict[str, str]) -> Dict[str, str]:
    """
    í–‰ì—ì„œ PII/ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    
    Args:
        row: DataFrame í–‰
        header_to_field: í—¤ë” -> Respondents í•„ë“œ ë§¤í•‘
        
    Returns:
        Dict[str, str]: Respondents í•„ë“œ ë°ì´í„°
    """
    pii_data = {
        "company": "",
        "department": "",
        "job_role": "",
        "tenure_years": "",
        "name": "",
        "phone": "",
        "email": "",
    }
    
    for header, value in row.items():
        header_lower = str(header).lower().strip()
        
        # PII ë§¤í•‘ì—ì„œ ì°¾ê¸°
        for pii_keyword, field_name in PII_COLUMN_MAPPING.items():
            if pii_keyword.lower() in header_lower:
                pii_data[field_name] = str(value).strip() if pd.notna(value) else ""
                break
    
    # íšŒì‚¬ëª… ì •ê·œí™”
    if pii_data["company"]:
        pii_data["company"] = normalize_company_name(pii_data["company"])
    
    return pii_data


def is_pii_column(header: str) -> bool:
    """
    í—¤ë”ê°€ PII/ë©”íƒ€ë°ì´í„° ì—´ì¸ì§€ íŒë‹¨
    
    Args:
        header: í—¤ë” í…ìŠ¤íŠ¸
        
    Returns:
        bool: PII ì—´ì´ë©´ True
    """
    header_lower = header.lower().strip()
    
    pii_keywords = [
        "íƒ€ì„ìŠ¤íƒ¬í”„", "timestamp", "ë‚ ì§œ", "date",
        "ì´ë¦„", "ì„±í•¨", "ì„±ëª…", "name",
        "ì „í™”", "ì—°ë½ì²˜", "phone", "mobile",
        "ì´ë©”ì¼", "ë©”ì¼", "email",
        "ì†Œì†", "íšŒì‚¬", "company",
        "ë¶€ì„œ", "department",
        "ì§êµ°", "ì§ë¬´", "ì§ì±…", "job",
        "ì—°ì°¨", "tenure",
    ]
    
    return any(keyword in header_lower for keyword in pii_keywords)


def normalize_header_text(text: str) -> str:
    """í—¤ë” ë° item_text ë¹„êµë¥¼ ìœ„í•œ ì •ê·œí™”"""

    normalized = str(text or "").lower()
    normalized = normalized.replace('"', "")
    normalized = normalized.replace("â€œ", "").replace("â€", "")
    normalized = normalized.replace("'", "")
    normalized = normalized.replace("[", "").replace("]", "")
    normalized = re.sub(r"\s+", " ", normalized)
    return normalized.strip()


def build_header_item_mapping(
    question_headers: List[str],
    registered_items: List[Dict],
) -> Tuple[Dict[str, str], List[str]]:
    """í—¤ë” í…ìŠ¤íŠ¸ë¥¼ item_idì— ë§¤í•‘"""

    header_lookup: Dict[str, str] = {}
    for header in question_headers:
        norm_header = normalize_header_text(header)
        header_lookup.setdefault(norm_header, header)

    header_to_item_id: Dict[str, str] = {}
    unmatched_headers = set(question_headers)
    used_item_ids = set()

    # 1) ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¼ì¹˜
    for item in registered_items:
        item_text = item.get("item_text", "")
        item_id = str(item.get("item_id", "") or "").strip()
        if not item_text or not item_id:
            continue

        item_norm = normalize_header_text(item_text)
        header = header_lookup.get(item_norm)
        if header and header not in header_to_item_id and item_id not in used_item_ids:
            header_to_item_id[header] = item_id
            used_item_ids.add(item_id)
            unmatched_headers.discard(header)

    # 2) ë¶€ë¶„ ë§¤ì¹­ (ì•ë¶€ë¶„ ë¹„êµ)
    if unmatched_headers:
        for header in list(unmatched_headers):
            h_norm = normalize_header_text(header)
            for item in registered_items:
                item_text = item.get("item_text", "")
                item_id = str(item.get("item_id", "") or "").strip()
                if not item_text or not item_id or item_id in used_item_ids:
                    continue

                item_norm = normalize_header_text(item_text)
                if h_norm and item_norm and (h_norm in item_norm or item_norm in h_norm):
                    header_to_item_id[header] = item_id
                    used_item_ids.add(item_id)
                    unmatched_headers.discard(header)
                    break

    return header_to_item_id, sorted(unmatched_headers)


# ============================================================================
# ë°ì´í„° ì£¼ì… ë©”ì¸ ë¡œì§
# ============================================================================

def inject_responses_for_course(
    spreadsheet: gspread.Spreadsheet,
    course_id: str,
    file_path: str,
    description: str,
):
    """íŠ¹ì • Courseì˜ ì‘ë‹µ ë°ì´í„° ì£¼ì…"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š ë°ì´í„° ì£¼ì… ì‹œì‘: {description} (ID: {course_id})")
    print(f"{'='*70}")
    
    # 1. íŒŒì¼ ì½ê¸°
    print(f"\n1ï¸âƒ£ íŒŒì¼ ì½ê¸°: {file_path}")
    try:
        df = read_response_file(file_path)
    except Exception as e:
        print(f"   âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        return
    
    if df.empty:
        print(f"   âš ï¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # 2. í—¤ë” ë¶„ì„
    print(f"\n2ï¸âƒ£ í—¤ë” ë¶„ì„ ({len(df.columns)}ê°œ ì»¬ëŸ¼)")
    headers = list(df.columns)

    question_headers = [h for h in headers if not is_pii_column(h)]
    pii_columns = [h for h in headers if is_pii_column(h)]

    if not question_headers:
        print("   âš ï¸ ë¬¸í•­ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # Survey_Items ë“±ë¡ ë° Course ë§¤í•‘ ì •ë¦¬
    try:
        registered_items = ensure_survey_items_from_headers(spreadsheet, question_headers)
        removed_count = delete_course_item_mappings(spreadsheet, course_id)
        ensure_course_item_mapping(spreadsheet, course_id, registered_items)
        print(f"   âœ… Survey_Items ë“±ë¡: {len(registered_items)}ê°œ (ê¸°ì¡´ ë§¤í•‘ {removed_count}ê°œ ì‚­ì œ í›„ ì¬ìƒì„±)")
    except Exception as e:
        print(f"   âŒ Survey_Items/ë§¤í•‘ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return

    header_to_item_id, unmatched_headers = build_header_item_mapping(question_headers, registered_items)

    question_columns = []
    for header in question_headers:
        item_id = header_to_item_id.get(header)
        if item_id:
            question_columns.append((header, item_id))
        else:
            print(f"   âš ï¸ ë§¤í•‘ ì‹¤íŒ¨: '{header[:50]}...'")

    if unmatched_headers:
        print(f"   âš ï¸ ë§¤ì¹­ë˜ì§€ ì•Šì€ í—¤ë”: {len(unmatched_headers)}ê°œ")
        for header in unmatched_headers[:5]:
            print(f"      - {header[:70]}")
        if len(unmatched_headers) > 5:
            print(f"      ... ì™¸ {len(unmatched_headers) - 5}ê°œ")

    print(f"   âœ… ë¬¸í•­ ì—´: {len(question_columns)}ê°œ")
    print(f"   âœ… PII ì—´: {len(pii_columns)}ê°œ")
    
    # 3. ë°ì´í„° ì£¼ì…
    print(f"\n3ï¸âƒ£ ë°ì´í„° ì£¼ì… ì‹œì‘ ({len(df)} ì‘ë‹µì)")
    
    batch_id = generate_batch_id()
    injected_responses = 0
    injected_respondents = 0
    
    for idx, row in df.iterrows():
        try:
            # ì‘ë‹µì ID ìƒì„±
            respondent_id = generate_respondent_id()
            
            # PII ì¶”ì¶œ ë° ì €ì¥
            pii_data = extract_pii_from_row(row, PII_COLUMN_MAPPING)
            respondent_data = {
                "respondent_id": respondent_id,
                "course_id": course_id,
                "pii_consent": "",
                "company": pii_data.get("company", ""),
                "department": pii_data.get("department", ""),
                "job_role": pii_data.get("job_role", ""),
                "tenure_years": pii_data.get("tenure_years", ""),
                "name": pii_data.get("name", ""),
                "phone": pii_data.get("phone", ""),
                "email": pii_data.get("email", ""),
                "hashed_contact": "",
                "extra_meta": "",
                "created_at": datetime.now(timezone.utc).isoformat(),
            }
            
            save_respondent(spreadsheet, respondent_data)
            injected_respondents += 1
            
            # ì‘ë‹µ ë°ì´í„° ì €ì¥
            for header, item_id in question_columns:
                answer_value = str(row[header]).strip() if pd.notna(row[header]) else ""
                
                if not answer_value or answer_value.lower() == "nan":
                    continue
                
                # ìˆ«ì ë³€í™˜ ì‹œë„
                response_value_num = None
                try:
                    response_value_num = float(answer_value)
                except (ValueError, TypeError):
                    pass
                
                response_data = {
                    "response_id": generate_response_id(),
                    "course_id": course_id,
                    "respondent_id": respondent_id,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "item_id": item_id,
                    "response_value": answer_value,
                    "response_value_num": response_value_num,
                    "choice_value": "",
                    "comment_text": answer_value,
                    "source_row_index": str(idx + 2),
                    "ingest_batch_id": batch_id,
                }
                
                save_response_v2(spreadsheet, response_data)
                injected_responses += 1
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            if (idx + 1) % 10 == 0:
                print(f"   â³ ì§„í–‰ ì¤‘: {idx + 1}/{len(df)} ì‘ë‹µì ì²˜ë¦¬...")
                time.sleep(0.5)  # API ì¿¼í„° ë³´í˜¸
            
        except Exception as e:
            print(f"   âš ï¸ í–‰ {idx + 2} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            continue
    
    print(f"\n   âœ… ì£¼ì… ì™„ë£Œ:")
    print(f"      - ì‘ë‹µì: {injected_respondents}ëª…")
    print(f"      - ì‘ë‹µ ë°ì´í„°: {injected_responses}ê°œ")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print("ğŸš€ NCT ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° ê°•ì œ ì£¼ì… ìŠ¤í¬ë¦½íŠ¸")
    print("="*70)
    
    # 1. Google Sheets ì—°ê²°
    print("\n1ï¸âƒ£ Google Sheets ì—°ê²° ì¤‘...")
    try:
        client = get_client()
        spreadsheet = open_or_create_spreadsheet(client)
        print(f"   âœ… ì—°ê²° ì„±ê³µ: {spreadsheet.title}")
    except Exception as e:
        print(f"   âŒ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return
    
    # 2. ì‹œíŠ¸ í´ë¦°ì§• (ìë™ ì‹¤í–‰)
    print("\n2ï¸âƒ£ ì‹œíŠ¸ í´ë¦°ì§• (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)")
    print("   âš ï¸ Responses ë° Respondents ì‹œíŠ¸ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
    print("   âœ… ìë™ ì‹¤í–‰ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    print("\n   ğŸ§¹ ì‹œíŠ¸ í´ë¦°ì§• ì¤‘...")
    clear_sheet_data(spreadsheet, "Responses")
    clear_sheet_data(spreadsheet, "Respondents")
    print("   âœ… í´ë¦°ì§• ì™„ë£Œ")
    
    # 3. ê° Courseë³„ ë°ì´í„° ì£¼ì…
    print("\n3ï¸âƒ£ ì‘ë‹µ ë°ì´í„° ì£¼ì… ì‹œì‘")
    
    for mapping in COURSE_FILE_MAPPING:
        try:
            inject_responses_for_course(
                spreadsheet=spreadsheet,
                course_id=mapping["course_id"],
                file_path=mapping["file_path"],
                description=mapping["description"],
            )
            
            # Course ê°„ ëŒ€ê¸° (API ì¿¼í„° ë³´í˜¸)
            print("\n   â¸ï¸ ë‹¤ìŒ Course ì²˜ë¦¬ ì „ 3ì´ˆ ëŒ€ê¸°...")
            time.sleep(3)
            
        except Exception as e:
            print(f"\n   âŒ {mapping['description']} ì£¼ì… ì‹¤íŒ¨: {str(e)}")
            continue
    
    # 4. ì™„ë£Œ
    print("\n" + "="*70)
    print("âœ… ëª¨ë“  ë°ì´í„° ì£¼ì… ì™„ë£Œ!")
    print("="*70)
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Streamlit ì•±ìœ¼ë¡œ ì´ë™")
    print("   2. ê´€ë¦¬ì ëª¨ë“œ â†’ DB ì„¤ì • â†’ ìºì‹œ í´ë¦¬ì–´")
    print("   3. ê´€ë¦¬ì ëª¨ë“œ â†’ ê³¼ì • ë¦¬ìŠ¤íŠ¸ â†’ ê²°ê³¼ ë³´ê¸°")
    print("   4. ë¶„ì„ ê²°ê³¼ í™•ì¸ (ğŸ“ˆ ê°ê´€ì‹, â­ í‰ì , ğŸ’¬ ì£¼ê´€ì‹)\n")


if __name__ == "__main__":
    main()

